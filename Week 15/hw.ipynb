{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7483403bdbd538",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# <a href=\"https://miptstats.github.io/courses/ad_mipt.html\">Phystech@DataScience</a>\n",
    "# Домашнее задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35242b34",
   "metadata": {},
   "source": [
    "**Правила, <font color=\"red\">прочитайте внимательно</font>:**\n",
    "\n",
    "* Выполненную работу нужно отправить телеграм-боту `@miptstats_pds_bot`. Для начала работы с ботом каждый раз отправляйте `/start`. **Работы, присланные иным способом, не принимаются.**\n",
    "* Дедлайн см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Прислать нужно ноутбук в формате `ipynb`.\n",
    "* Выполнять задание необходимо полностью самостоятельно. **При обнаружении списывания все участники списывания будут сдавать устный зачет.**\n",
    "* Решения, размещенные на каких-либо интернет-ресурсах, не принимаются. Кроме того, публикация решения в открытом доступе может быть приравнена к предоставлении возможности списать.\n",
    "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него. Можно добавлять необходимое количество ячеек.\n",
    "* Комментарии к решению пишите в markdown-ячейках.\n",
    "* Выполнение задания (ход решения, выводы и пр.) должно быть осуществлено на русском языке.\n",
    "* Если код будет не понятен проверяющему, оценка может быть снижена.\n",
    "* Никакой код из данного задания при проверке запускаться не будет. *Если код студента не выполнен, недописан и т.д., то он не оценивается.*\n",
    "* **Код из рассказанных на занятиях ноутбуков можно использовать без ограничений.**\n",
    "\n",
    "**Правила оформления теоретических задач:**\n",
    "\n",
    "* Решения необходимо прислать одним из следующих способов:\n",
    "  * фотографией в правильной ориентации, где все четко видно, а почерк разборчив,\n",
    "    * отправив ее как файл боту вместе с ноутбуком *или*\n",
    "    * вставив ее в ноутбук посредством `Edit -> Insert Image` (<font color=\"red\">фото, вставленные ссылкой, не принимаются</font>);\n",
    "  * в виде $\\LaTeX$ в markdown-ячейках.\n",
    "* Решения не проверяются, если какое-то требование не выполнено. Особенно внимательно все проверьте в случае выбора второго пункта (вставки фото в ноутбук). <font color=\"red\"><b>Неправильно вставленные фотографии могут не передаться при отправке.</b></font> Для проверки попробуйте переместить `ipynb` в другую папку и открыть его там.\n",
    "* В решениях поясняйте, чем вы пользуетесь, хотя бы кратко. Например, если пользуетесь независимостью, то достаточно подписи вида \"*X и Y незав.*\"\n",
    "* Решение, в котором есть только ответ, и отсутствуют вычисления, оценивается в 0 баллов.\n",
    "\n",
    "**Баллы за задание:**\n",
    "\n",
    "* Задача 1 &mdash;  50 баллов\n",
    "* Задача 2 &mdash;  30 баллов\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2ebd39891ec11",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "from IPython.display import Image, clear_output\n",
    "from collections import defaultdict\n",
    "from torch.optim import lr_scheduler \n",
    "from matplotlib.animation import FuncAnimation, ImageMagickFileWriter\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8dab69b24a754b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Задача 1 (**Профиль физика**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a3f7f",
   "metadata": {},
   "source": [
    "* Реализуйте обучение и тестирование нейронной сети из прошлого домашнего задания с использованием PyTorch Lightning и Tensorboard. \n",
    "* Улучшите архитектуру сети с учетом новых знаний.\n",
    "* Проведите минимум 7 экспериментов для определения наилучших параметров обучения. Например, можно менять вероятность отключения нейронов в слоях Dropout, число нейронов на скрытых слоях, learning rate. Возможно, вы захотите воспользоваться learning rate scheduler, а также проверить разные оптимизаторы.\n",
    "\n",
    "**Полезные ссылки:**\n",
    "[Lightning callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html)\n",
    "[Lightning learning rate schedulers](https://pytorch.org/docs/2.4/optim.html#how-to-adjust-learning-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e471566",
   "metadata": {},
   "source": [
    "Скачайте [датасет](https://disk.yandex.ru/d/LighhESdTbYk6Q), описывающий распады Z-бозонов двух типов: `Zee`и `Zmumu`. Создайте и обучите нейросеть, разделяющую эти два класса.\n",
    "\n",
    "[Описание датасета](https://www.kaggle.com/datasets/omidbaghchehsaraei/identification-of-two-modes-of-z-boson?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896d2cc095146d1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8124b6195f378a68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Удалите столбцы 'Unnamed: 0\t', 'Run' и 'Event', так как это не физические величины. Удалите строки, где есть пропуски, если таковые имеются."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c8233798d0c6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48293c57ce0eb242",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Убедитесь, что классы сбалансированны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10585fe5e563c9e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba2187744243ea97",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Также можно как-нибудь взглянуть на признаки. Возможно, не все они вносят вклад в разделение классов. Не забудьте преобразовать таргет (столбец 'class') к формату 0 и 1. Вам может пригодиться `sklearn.preprocessing.LabelBinarizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7eaeadeeebcee3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090067867437553",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5a47b6477c88cba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Выводы:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb763efb47c38b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e44d80a24ff45d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Задача 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626d28d1bf7d8d5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "На семинаре вы уже познакомились с основными методами оптимизации, которые широко используются в классическом машинном обучении. С развитием нейронных сетей и активным внедрением нейросетевого подхода, методы оптимизации стали ещё более актуальными. Но стандартные методы оптимизации, SGD и метод тяжёлого шара, имеют ряд недостатков, из-за чего их редко применяют в чистом виде. Для обучения современных нейросетей используют более продвинутые методы. \n",
    "\n",
    "В данной задаче вам предстоит самостоятельно реализовать различные оптимизаторы (запущенные из одной точки) и сравнить скорости их сходимости.\n",
    "\n",
    "Пусть задача оптимизации имеет вид $f(x) \\longrightarrow \\min\\limits_x$, и $\\nabla_{x} f(x)$ &mdash; градиент функции $f(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca84e6fd81e2d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! apt-get install imagemagick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e8069ba0ee8ac7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ecfe7",
   "metadata": {},
   "source": [
    "Обычный и стохастический градиентный спуск.\n",
    "\n",
    "$$x_{t + 1} = x_t - \\eta v_t,$$\n",
    "\n",
    "где $v_{t} = \\nabla f(x_{t})$ &mdash; аналогия со скоростью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72455f186c99f35d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd(init_parameters, func_grad, lr, n_iter):\n",
    "    '''\n",
    "    Метод оптимизации SGD.\n",
    "    \n",
    "    Параметры:\n",
    "    - parameters - начальное приближение параметров,\n",
    "    - func_grad - функция, задающая градиент оптимизируемой функции,\n",
    "    - lr - скорость обучения,\n",
    "    - n_iter - количество итераций метода.\n",
    "    \n",
    "    Возвращает историю обновлений параметров.\n",
    "    '''\n",
    "    \n",
    "    parameters = init_parameters.copy()\n",
    "    history = [parameters.copy()]\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # ваш код\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c276bcda720dfd8c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. SGD + Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37256288",
   "metadata": {},
   "source": [
    "Сгладим градиент, используя информацию о том, как градиент изменялся раньше. \n",
    "Физическая аналогия &mdash; добавляем инерцию.\n",
    "\n",
    "\n",
    "$$x_{t + 1} = x_t + v_{t},$$\n",
    "где $v_{t} = \\mu v_{t - 1} - \\eta \\nabla f(x_{t})$ &mdash; сглаживаем градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de341b7965aff0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_momentum(init_parameters, func_grad, lr, mu, n_iter):\n",
    "    '''\n",
    "    Метод оптимизации SGD Momentum.\n",
    "    \n",
    "    Параметры:\n",
    "    - parameters - начальное приближение параметров,\n",
    "    - func_grad - функция, задающая градиент оптимизируемой функции,\n",
    "    - lr - скорость обучения,\n",
    "    - mu - коэффициент сглаживания,\n",
    "    - n_iter - количество итераций метода.\n",
    "    \n",
    "    Возвращает историю обновлений параметров.\n",
    "    '''\n",
    "    parameters = init_parameters.copy()\n",
    "    history = [parameters.copy()]\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # ваш код\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2831b7e1d3c763f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a45b99",
   "metadata": {},
   "source": [
    "Adagrad &mdash; один из самых первых адаптивных методов оптимизации.\n",
    "\n",
    "Во всех изученных ранее методах есть необходимость подбирать шаг метода (коэффициент $\\eta$). На каждой итерации все компоненты градиента оптимизируемой функции домножаются на одно и то же число $\\eta$. Но использовать одно значение $\\eta$ для всех параметров не оптимально, так как они имеют различные распределения и оптимизируемая функция изменяется с совершенно разной скоростью при небольших изменениях разных параметров. \n",
    "\n",
    "Поэтому гораздо логичнее **изменять значение каждого параметра с индивидуальной скоростью**. При этом, *чем c большей степени от изменения параметра меняется значение оптимизируемой функции, тем с меньшей скоростью стоить обновлять этот параметр*. Иначе высок шанс расходимости метода. Получить такой результат удается, если разделить градиент на сумму квадратов скорости изменений параметров.\n",
    "\n",
    "Пусть $x^{(i)}$ &mdash; $i$-я компонента вектора $x$.\n",
    "$$x_{t+1, i} = x_{t, i} - \\frac{\\eta}{\\sqrt{g_{t, i}+\\varepsilon}}\\cdot \\nabla f_i(x_t)$$\n",
    "$$g_{t} = g_{t-1} + \\nabla f(x_t) \\odot \\nabla f(x_t)$$\n",
    "\n",
    "\n",
    "В матрично-векторном виде шаг алгоритма можно переписать так:\n",
    "$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla f(x_t).$$\n",
    "Здесь $\\odot$ обозначает произведение Адамара, т.е. поэлементное перемножение векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd4600966dd1f44",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adagrad(init_parameters, func_grad, lr, eps, n_iter):\n",
    "    '''\n",
    "    Метод оптимизации Adagrad.\n",
    "    \n",
    "    Параметры:\n",
    "    - parameters - начальное приближение параметров,\n",
    "    - func_grad - функция, задающая градиент оптимизируемой функции,\n",
    "    - lr - скорость обучения,\n",
    "    - eps - минимальное значение нормирующего члена,\n",
    "    - n_iter - количество итераций метода.\n",
    "    \n",
    "    Возвращает историю обновлений параметров.\n",
    "    '''\n",
    "    parameters = init_parameters.copy()\n",
    "    history = [parameters.copy()]\n",
    "    \n",
    "    for iter_id in range(n_iter):\n",
    "        \n",
    "        # ваш код\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e510a01ac52613d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b79727",
   "metadata": {},
   "source": [
    "Алгоритм RMSProp основан на той же идее, что и алгоритм Adagrad &mdash; адаптировать learning rate отдельно для каждого параметра $\\theta^{(i)}$.  Однако Adagrad имеет серьёзный недостаток. Он с одинаковым весом учитывает квадраты градиентов как с самых первых итераций, так и с самых последних. Хотя, на самом деле, наибольшую значимость имеют модули градиентов на последних нескольких итерациях. \n",
    "\n",
    "Для этого предлагается использовать **экспоненциальное сглаживание**.\n",
    "$$x_{t+1} = x_{t} - \\frac{\\eta}{\\sqrt{g_{t} + \\varepsilon}}\\odot \\nabla f(x_t).$$\n",
    "$$g_t = \\mu g_{t-1} + (1-\\mu) \\nabla f(x_t) \\odot \\nabla f(x_t)$$\n",
    "\n",
    "Стандартные значения гиперпараметров: $\\mu = 0.9, \\eta = 0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5032dc7e33dc92",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rmsprop(init_parameters, func_grad, lr, mu, eps, n_iter):\n",
    "    '''\n",
    "    Метод оптимизации RMSProp.\n",
    "    \n",
    "    Параметры:\n",
    "    - parameters - начальное приближение параметров,\n",
    "    - func_grad - функция, задающая градиент оптимизируемой функции,\n",
    "    - lr - скорость обучения,\n",
    "    - mu - коэффициент сглаживания,\n",
    "    - eps - минимальное значение нормирующего члена,\n",
    "    - n_iter - количество итераций метода.\n",
    "    \n",
    "    Возвращает историю обновлений параметров.\n",
    "    '''\n",
    "    \n",
    "    parameters = init_parameters.copy()\n",
    "    history = [parameters.copy()]\n",
    "\n",
    "    for iter_id in range(n_iter):\n",
    "        \n",
    "        # ваш код\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad7e666e95017",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd50ca5",
   "metadata": {},
   "source": [
    "Этот алгоритм совмещает в себе 2 идеи: \n",
    "* идею алгоритма Momentum *о накапливании градиента*, \n",
    "* идею методов Adadelta и RMSProp *об экспоненциальном сглаживании* информации о предыдущих значениях квадратов градиентов.\n",
    "\n",
    "Благодаря использованию этих двух идей, метод имеет 2 преимущества над большей частью методов первого порядка, описанных выше:\n",
    "\n",
    "\n",
    "1. Он обновляет все параметры $\\theta$ не с одинаковым `learning rate`, а выбирает для каждого $\\theta_i$ индивидуальный `learning rate`, что *позволяет учитывать разреженные признаки с большим весом*.\n",
    "\n",
    "\n",
    "2. Adam за счёт применения экспоненциального сглаживания к градиенту *работает более устойчиво в окрестности оптимального значения параметра $\\theta^*$*, чем методы, использующие градиент в точке $x_t$, не накапливая значения градиента с прошлых шагов.\n",
    "\n",
    "\n",
    "Формулы шага алгоритма выглядят так:\n",
    "$$v_{t + 1} = \\beta v_{t} + (1-\\beta) \\nabla x(x_{t})$$\n",
    "$$g_t = \\mu g_{t-1} + (1-\\mu) \\nabla x(x_t) \\odot \\nabla x(x_t)$$\n",
    "\n",
    "Чтобы эти оценки не были смещёнными, нужно их отнормировать:\n",
    "$$\\widehat{v}_{t + 1} = \\frac{v_{t + 1}}{1-\\beta^t},$$\n",
    "$$\\widehat{g}_t = \\frac{g_t}{1-\\mu^t}.$$\n",
    "\n",
    "Тогда получим итоговую формулу шага:\n",
    "\n",
    "$$x_{t+1} = x_t - \\frac{\\eta}{\\sqrt{\\widehat{g}_t + \\varepsilon}} \\odot \\widehat{v}_{t + 1}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a354879cbf02d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adam(init_parameters, func_grad, eps, lr, beta, mu,  n_iter):\n",
    "    '''\n",
    "    Adam.\n",
    "    \n",
    "    Параметры.\n",
    "    1) theta0 - начальное приближение theta,\n",
    "    2) func_grad - функция, задающая градиент оптимизируемой функции,\n",
    "    3) eps - мин. возможное значение знаменателя,\n",
    "    4) lr - скорость обучения,\n",
    "    5) beta - параметр экспоненциального сглаживания,\n",
    "    6) mu - параметр экспоненциального сглаживания,\n",
    "    7) n_iter - количество итераций метода.\n",
    "    '''\n",
    "    \n",
    "    parameters = init_parameters.copy()\n",
    "    history = [parameters.copy()]\n",
    "    \n",
    "    for iter_id in range(n_iter):\n",
    "        \n",
    "        # ваш код\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835413d",
   "metadata": {},
   "source": [
    "### Сравнение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141db75e059027fd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Сравнение оптимизаторов будем производить на примере двух функций:\n",
    "1. $f(x, y) = 5*x^2 + y^2$\n",
    "2. $f(x, y) = (x-3)^2 + 8(y-5)^4 + \\sqrt{x} + \\sin(xy)$\n",
    "\n",
    "Реализуйте данные функции в ячейке ниже для удобства и читаемости кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86099ed6909106cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def square_sum(x):\n",
    "    ''' f(x, y) = 5 * x^2 + y^2 '''\n",
    "    \n",
    "    return # ваш код\n",
    "\n",
    "def square_sum_grad(x):\n",
    "    ''' grad f(x, y) = <..> '''\n",
    "    \n",
    "    return # ваш код\n",
    "\n",
    "\n",
    "def complex_sum(x):\n",
    "    ''' f(x, y) = (x-3)^2 + 8(y-5)^4 + sqrt(x) + sin(xy)'''\n",
    "    \n",
    "    return # ваш код\n",
    "\n",
    "def complex_sum_grad(x):\n",
    "    ''' grad f(x, y) = <..> '''\n",
    "    \n",
    "    partial_x = # ваш код\n",
    "    partial_y = # ваш код\n",
    "                                                                   \n",
    "    return np.array([partial_x, partial_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f4c75ca9819f42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Создадим директорию, в которой будем хранить визуализацию экспериментов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d83baf3bd404c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -rf saved_gifs\n",
    "!mkdir saved_gifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ccb53b75bbdd5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Напишем функцию, которая будет отрисовывать процесс оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17a76a824e900e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_experiment(func, trajectory, graph_title,\n",
    "                    min_y=-7, max_y=7, min_x=-7, max_x=7):\n",
    "    '''\n",
    "    Функция, которая для заданной функции рисует её линии уровня, \n",
    "    а также траекторию сходимости метода оптимизации.\n",
    "    \n",
    "    Параметры.\n",
    "    1) func - оптимизируемая функция,\n",
    "    2) trajectory - траектория метода оптимизации,\n",
    "    3) graph_name - заголовок графика.\n",
    "    '''\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    xdata, ydata = [], []\n",
    "    ln, = plt.plot([], [])\n",
    "    \n",
    "    mesh_x = np.linspace(min_x, max_x, 300)\n",
    "    mesh_y = np.linspace(min_y, max_y, 300)\n",
    "    X, Y = np.meshgrid(mesh_x, mesh_y)\n",
    "    Z = np.zeros((len(mesh_x), len(mesh_y)))\n",
    "    \n",
    "    for coord_x in range(len(mesh_x)):\n",
    "        for coord_y in range(len(mesh_y)):\n",
    "            Z[coord_y, coord_x] = func(\n",
    "                np.array((mesh_x[coord_x], \n",
    "                          mesh_y[coord_y]))\n",
    "            )\n",
    "    \n",
    "    def init():\n",
    "        ax.contour(\n",
    "            X, Y, np.log(Z), \n",
    "            np.log([0.5, 10, 30, 80, 130, 200, 300, 500, 900]), \n",
    "            cmap='winter'\n",
    "        )\n",
    "        ax.set_title(graph_title)\n",
    "        return ln,\n",
    "\n",
    "    def update(frame):\n",
    "        xdata.append(trajectory[frame][0])\n",
    "        ydata.append(trajectory[frame][1])\n",
    "        ln.set_data(xdata, ydata)\n",
    "        return ln,\n",
    "    \n",
    "    ani = FuncAnimation(\n",
    "        fig, update, frames=range(len(trajectory)),\n",
    "        init_func=init, repeat=True\n",
    "    )\n",
    "    writer = ImageMagickFileWriter(fps=10)\n",
    "    ani.save(f'saved_gifs/{graph_title}.gif', \n",
    "             writer=writer)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001c5262c79268a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Простая функция $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f479553797b94941",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = np.array((5, 5), dtype=float)\n",
    "func_name = '$f(x, y) = 5x^2 + y^2$'\n",
    "func_grad = square_sum_grad\n",
    "func = square_sum\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc60500b119dae4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f04d5a44ae56f8c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_trajectory = sgd(\n",
    "    init_parameters=parameters, \n",
    "    func_grad=func_grad, \n",
    "    lr=0.01, \n",
    "    n_iter=n_iter\n",
    ")\n",
    "graph_title = 'Траектория SGD, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    sgd_trajectory, \n",
    "    graph_title,\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa37078b393866d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2042927e2c67fd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_momentum_trajectory = sgd_momentum(\n",
    "    init_parameters=parameters.copy(), \n",
    "    func_grad=func_grad, \n",
    "    lr=0.01, \n",
    "    n_iter=n_iter,\n",
    "    mu=0.9\n",
    ")\n",
    "graph_title = 'Траектория SGD Momentum, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    sgd_momentum_trajectory, \n",
    "    graph_title,\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c559dd6918f6da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fee3383ffc4c78",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adagrad_trajectory = adagrad(\n",
    "    init_parameters=parameters.copy(),  \n",
    "    func_grad=func_grad, \n",
    "    lr=0.1, \n",
    "    n_iter=n_iter,\n",
    "    eps=1e-6,\n",
    ")\n",
    "graph_title = 'Траектория Adagrad, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    adagrad_trajectory, \n",
    "    graph_title,\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae721bdd18942cb0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af148aee120c5fae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmsprop_trajectory = rmsprop(\n",
    "    init_parameters=parameters.copy(),  \n",
    "    func_grad=func_grad, \n",
    "    lr=0.1, \n",
    "    n_iter=n_iter,\n",
    "    eps=1e-6,\n",
    "    mu=0.9\n",
    ")\n",
    "graph_title = 'Траектория RMSProp, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    rmsprop_trajectory, \n",
    "    graph_title,\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7349b2f5431485",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf449c41312013",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam_trajectory = adam(\n",
    "    init_parameters=parameters.copy(),  \n",
    "    func_grad=func_grad, \n",
    "    lr=0.1, \n",
    "    n_iter=n_iter,\n",
    "    eps=1e-6,\n",
    "    mu=0.9,\n",
    "    beta=0.9\n",
    ")\n",
    "graph_title = 'Траектория Adam, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    adam_trajectory, \n",
    "    graph_title,\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a6ed8a7fbca89e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593c64d3d5082c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Сложная функция $f(x, y) = (x-3)^2 + 8(y-5)^4 + \\sqrt{x} + \\sin(xy)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dcc5a6937a011",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = np.array((5, -2), dtype=float)\n",
    "func_name = '$f(x, y) = (x-3)^2 + 8(y-5)^4 + \\sqrt{x} + \\sin(xy)$'\n",
    "func_grad = complex_sum_grad\n",
    "func = complex_sum\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e6d57de625557",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326b3f3b72148d3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_trajectory = sgd(\n",
    "    init_parameters=parameters, \n",
    "    func_grad=func_grad, \n",
    "    lr=0.0002, \n",
    "    n_iter=n_iter\n",
    ")\n",
    "graph_title = 'Траектория SGD, ' +  func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    sgd_trajectory, \n",
    "    graph_title, \n",
    "    -5, 10, 0, 10\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de352da5e105ecdf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### SGD Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2004321608fa348e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd_momentum_trajectory = sgd_momentum(\n",
    "    init_parameters=parameters.copy(), \n",
    "    func_grad=func_grad, \n",
    "    lr=0.0002, \n",
    "    n_iter=n_iter,\n",
    "    mu=0.7\n",
    ")\n",
    "graph_title = 'Траектория SGD Momentum, ' +  func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    sgd_momentum_trajectory, \n",
    "    graph_title,\n",
    "    -5, 10, 0, 10\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5653aafdd1dadd5b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522338acf42e633",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adagrad_trajectory = adagrad(\n",
    "    init_parameters=parameters.copy(),  \n",
    "    func_grad=func_grad, \n",
    "    lr=0.1, \n",
    "    n_iter=n_iter,\n",
    "    eps=1e-6,\n",
    ")\n",
    "graph_title = 'Траектория Adagrad, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    adagrad_trajectory, \n",
    "    graph_title,\n",
    "    -5, 10, 0, 10\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1042830fcb1604",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc493a0575c6955b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmsprop_trajectory = rmsprop(\n",
    "    init_parameters=parameters.copy(),  \n",
    "    func_grad=func_grad, \n",
    "    lr=0.1, \n",
    "    n_iter=n_iter,\n",
    "    eps=1e-6,\n",
    "    mu=0.9\n",
    ")\n",
    "graph_title = 'Траектория RMSProp, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    rmsprop_trajectory, \n",
    "    graph_title,\n",
    "    -5, 10, 0, 10\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86273963ad16b5ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd1c97bf80929a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adam_trajectory = adam(\n",
    "    init_parameters=parameters.copy(),  \n",
    "    func_grad=func_grad, \n",
    "    lr=0.1, \n",
    "    n_iter=n_iter,\n",
    "    eps=1e-6,\n",
    "    mu=0.9,\n",
    "    beta=0.9\n",
    ")\n",
    "graph_title = 'Траектория Adam, ' + func_name\n",
    "make_experiment(\n",
    "    func, \n",
    "    adam_trajectory, \n",
    "    graph_title,\n",
    "    -5, 10, 0, 10\n",
    ")\n",
    "clear_output()\n",
    "Image(open(f'saved_gifs/{graph_title}.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eed9343b39310a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898899327618353b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
